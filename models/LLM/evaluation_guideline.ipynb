{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Guideline\n",
    "\n",
    "## 1. Set Up the Evaluation Framework\n",
    "- **Objective**: Establish a robust method to assess AI model performance.\n",
    "- **Key Considerations**:\n",
    "  - Focus on functional correctness as the ultimate metric for evaluating application performance.\n",
    "  - Use a combination of reference-based and reference-free metrics depending on available data.\n",
    "## 2. Core Evaluation Metrics\n",
    "- **Loss Function**:\n",
    "  - Return logarithmic loss to quantify model performance on training data.\n",
    "- **Functional Correctness**:\n",
    "  - Measure how well the AI performs its intended task (e.g., answering questions accurately).\n",
    "\n",
    "## 3. Similarity Measurements (Against Reference Data)\n",
    "- **Input**: Question, generated response, and reference response(s).\n",
    "- **Types of Similarity**:\n",
    "  1. **Lexical Similarity**:\n",
    "     - Measures how similar the generated response *looks* to the reference.\n",
    "     - Metrics: BLEU, ROUGE, METEOR++, TER, CIDEr (n-gram-based).\n",
    "     - **Caveats**: Requires a comprehensive set of reference responses; higher scores don’t always mean better responses.\n",
    "  2. **Semantic Similarity**:\n",
    "     - Evaluates closeness in meaning between generated and reference responses.\n",
    "     - Depends on the quality of embedding algorithms; poor embeddings can yield low scores despite similar meanings.\n",
    "     - **Drawback**: May require significant compute and time.\n",
    "- **Avoid**: Exact match (rarely works effectively).\n",
    "\n",
    "## 4. AI as a Judge\n",
    "- **Overview**: AI-judged evaluation is a prevalent method in production settings.\n",
    "- **Approaches**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. **Quality of Response**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given the following question and answer, evaluate how good the answer is\n",
    "    for the question. Use the score from 1 to 5.\n",
    "    - 1 means very bad.\n",
    "    - 5 means very good.\n",
    "    Question: [QUESTION]\n",
    "    Answer: [ANSWER]\n",
    "    Score:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Comparison to Reference**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given the following question, reference answer, and generated answer,\n",
    "    evaluate whether this generated answer is the same as the reference answer.\n",
    "    Output True or False.\n",
    "    Question: [QUESTION]\n",
    "    Reference answer: [REFERENCE ANSWER]\n",
    "    Generated answer: [GENERATED ANSWER]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. **Preference Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given the following question and two answers, evaluate which answer is\n",
    "    better. Output A or B.\n",
    "    Question: [QUESTION]\n",
    "    A: [FIRST ANSWER]\n",
    "    B: [SECOND ANSWER]\n",
    "    The better answer is:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Prompting an AI Judge**:\n",
    "  1. Define the task (e.g., evaluate relevance).\n",
    "  2. Specify criteria (e.g., “Does the answer address the question per the ground truth?”).\n",
    "  3. Choose a scoring system:\n",
    "     - **Classification**: e.g., good/bad, relevant/irrelevant.\n",
    "     - **Discrete Numerical**: e.g., 1-5 (preferred over continuous).\n",
    "     - **Continuous Numerical**: e.g., 0-1 (less effective).\n",
    "  - **Tips**:\n",
    "    - Language models excel with text/classification over numbers.\n",
    "    - Discrete scoring (1-5) outperforms wider ranges or continuous scoring.\n",
    "    - Include examples in prompts (e.g., what a 1, 3, or 5 looks like) for better results.\n",
    "\n",
    "- **Example Prompt**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  Score the relevance of a generated answer to a question based on the ground truth, using 1-5. Focus on whether the generated answer contains sufficient info per the ground truth. If it contradicts the ground truth, score 1-2. \n",
    "  Example: \n",
    "  Question: \"Is the sky blue?\" \n",
    "  Ground truth: \"Yes, the sky is blue.\" \n",
    "  Generated: \"No, the sky is not blue.\" \n",
    "  Score: 1 (Reason: Contradicts ground truth).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Specialized Evaluation Tools\n",
    "- **Reward Model**:\n",
    "- Scores a (prompt, response) pair based on quality.\n",
    "- **Reference-Based Judge**:\n",
    "- Assesses generated responses relative to reference(s).\n",
    "- **Preference Model**:\n",
    "- Predicts user-preferred responses for alignment or ranking.\n",
    "- **Self-Evaluation/Self-Critique**:\n",
    "- Model evaluates its own outputs (experimental approach).\n",
    "- **Subset Evaluation**:\n",
    "- Use a strong model (e.g., GPT-4) to judge a sample (e.g., 1%) of responses generated by a cheaper model.\n",
    "\n",
    "## 6. Deprecated/Unsupported Metrics\n",
    "- **Perplexity** *(Don’t Use)*:\n",
    "- Lower with structured data, higher with large vocabularies, lower with longer contexts.\n",
    "- Typical values can be as low as 3, but it’s unreliable for models post-trained with SFT/RLHF.\n",
    "- Maybe use to confirm the data, model from pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Finetuning\n",
    "1. **Your training data should be:**\n",
    "- Large (ideally thousands or tens of thousands of examples)\n",
    "- High-quality (consistently formatted and cleaned of incomplete or incorrect 0 examples)\n",
    "- Representative (training data should be similar to the data upon which you’ll use your model)\n",
    "- Sufficiently specified (i.e., containing enough information in the input to generate what you want to see in the output)\n",
    "\n",
    "2. **Prompts for a fine-tuned model do not typically need instructions or examples, as the model can learn the task from the training examples. Including instructions shouldn’t hurt performance, but the extra text tokens will add cost to each API call.**\n",
    "\n",
    "3. **Instructions can still be useful when fine-tuning a single model to do multiple tasks. For example, if you train a model to classify multiple features from the same text string (e.g., whether an item is edible or whether it’s handheld), you’ll need some type of instruction to tell the model which feature you want labeled.\n",
    "\n",
    "4. For classification, end your text prompts with a text sequence to tell the model that the input text is done and the classification should begin. Without such a signal, the model may append additional invented text before appending a class label, resulting in outputs like:\n",
    "- burger edible (accurate)\n",
    "- burger and fries edible (not quite was asked for)\n",
    "- burger-patterned novelty tie inedible (inaccurate)\n",
    "- burger burger burger burger (no label generated)\n",
    "\n",
    "5. In general, fine-tuning can work with any label, whether the label has semantic meaning (e.g., “ edible”) or not (e.g., “1”). That said, in cases with little training data per label, it’s possible that semantic labels work better, so that the model can leverage its knowledge of the label’s meaning.\n",
    "\n",
    "    When convenient, we recommend single-token labels. You can check the number of tokens in a string with the OpenAI tokenizer. Single-token labels have a few advantages:\n",
    "Lowest cost\n",
    "Easier to get their probabilities, which are useful for metrics confidence scores, precision, recall\n",
    "No hassle from specifying stop sequences or post-processing completions in order to compare labels of different length\n",
    "\n",
    "6. One useful fact: all numbers <500 are single tokens.\n",
    "7. If you do use multi-token labels, we recommend that each label begin with a different token. If multiple labels begin with the same token, an unsure model might end up biased toward those labels.\n",
    "8. To assess the value of getting more data, you can train models on subsets of your current dataset—e.g., 25%, 50%, 100%—and then see how performance scales with dataset size. If you plot accuracy versus number of training examples, the slope at 100% will indicate the improvement you can expect from getting more data. (Note that you cannot infer the value of additional data from the evolution of accuracy during a single training run, as a model half-trained on twice the data is not equivalent to a fully trained model.) \n",
    "9. Evaluating your fine-tuned model is crucial to (a) improve your model and (b) tell when it’s good enough to be deployed.\n",
    "\n",
    "    Many metrics can be used to characterize the performance of a classifier\n",
    "    - Accuracy\n",
    "    - F1\n",
    "    - Precision / Positive Predicted Value / False Discovery Rate\n",
    "    - Recall / Sensitivity\n",
    "    - Specificity\n",
    "    - AUC / AUROC (area under the receiver operator characteristic curve)\n",
    "    - AUPRC (area under the precision recall curve)\n",
    "    - Cross entropy\n",
    "10. A single project might end up trying all models. One illustrative development path might look like this:\n",
    "    - Test code using the cheapest & fastest model (ada)\n",
    "    - Run a few early experiments to check whether your dataset works as expected with a middling model (curie)\n",
    "    - Run a few more experiments with the best model to see how far you can push performance (text-davinci-002)\n",
    "    - Once you have good results, do a training run with all models to map out the price-performance frontier and select the model that makes the most sense for your use case  (ada, babbage, curie, davinci, text-davinci-002)\n",
    "11. Another possible development path that uses multiple models could be:\n",
    "    - Starting with a small dataset, train the best possible model (text-davinci-002)\n",
    "    - Use this fine-tuned model to generate many more labels and expand your dataset by multiples\n",
    "    - Use this new dataset to train a cheaper model (ada)\n",
    "12. - Step 1: Fine-tune on cheap, semi-relevant data\n",
    "    - E.g., unstructured domain text (such as legal or medical text)\n",
    "    - E.g., similar task data (such as another large classification set)\n",
    "    - Step 2: Fine-tune on expensive labeled examples\n",
    "    - E.g., text and classes (if training a classifier)\n",
    "    - To fine-tune a previously fine-tuned model, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft-<org>-<date>). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common mistakes\n",
    "1. **Insufficiently specified training data**:\n",
    "One thing to keep in mind is that training data is more than just a mapping of inputs to correct answers. Crucially, the inputs need to contain the information needed to derive an answer. This can happen more subtly when some information is given but some is still missing.\n",
    "2. **Input data format that doesn’t match the training data format**:\n",
    "Make sure that when you use your fine-tuned model, your submitted prompts match the format of your training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prompt Engineering\n",
    "1. **Write Clear and Explicit Instructions**:\n",
    "Communicating with AI is the same as communicating with humans: clarity helps.\n",
    "Here are a few tips on how to write clear instructions.\n",
    "    - Explain, without ambiguity, what you want the model to do\n",
    "If you want the model to score an essay, explain the score system you want to use. Is\n",
    "it from 1 to 5 or 1 to 10? If there’s an essay the model’s uncertain about, do you want\n",
    "it to pick a score to the best of its ability or to output “I don’t know”?\n",
    "    - As you experiment with a prompt, you might observe undesirable behaviors that\n",
    "require adjustments to the prompt to prevent them.\n",
    "\n",
    "2. **Ask the model to adopt a persona**:\n",
    "A persona can help the model to understand the perspective it’s supposed to use to\n",
    "generate responses.\n",
    "\n",
    "3. **Provide examples**:\n",
    "Examples can reduce ambiguity about how you want the model to respond. This might sound obvious, but if you’re worried about input token length, opt for\n",
    "example formats that use fewer tokens.\n",
    "\n",
    "4. **Specify the output format**:\n",
    "If you want the model to be concise, tell it so. Long outputs are not only costly\n",
    "(model APIs charge per token) but they also increase latency. For tasks expecting structured outputs, such as classification, use markers to mark\n",
    "the end of the prompts to let the model know that the structured outputs should\n",
    "begin.\n",
    "\n",
    "5. **Provide Sufficient Context**:\n",
    "Just as reference texts can help students do better on an exam, sufficient context can\n",
    "help models perform better. If you want the model to answer questions about a\n",
    "paper, including that paper in the context will likely improve the model’s responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More reference\n",
    "- **OPENAI Cook book for finetuning** https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit?tab=t.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
